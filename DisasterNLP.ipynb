{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('train.csv')\n",
    "dataset.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In our Dataset we have  0.8 % of keywords missing, and  33.27 % of location data missing\n"
     ]
    }
   ],
   "source": [
    "keywords_missing = dataset['keyword'].isna().sum()/dataset['keyword'].isna().count()*100\n",
    "location_missing = dataset['location'].isna().sum()/dataset['location'].isna().count()*100\n",
    "\n",
    "print('In our Dataset we have ', keywords_missing.round(2), '% of keywords missing, and ', location_missing.round(2),'% of location data missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>nothing</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>nothing</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>nothing</td>\n",
       "      <td>unknown</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>nothing</td>\n",
       "      <td>unknown</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>nothing</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  keyword location                                               text  \\\n",
       "0   1  nothing  unknown  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4  nothing  unknown             Forest fire near La Ronge Sask. Canada   \n",
       "2   5  nothing  unknown  All residents asked to 'shelter in place' are ...   \n",
       "3   6  nothing  unknown  13,000 people receive #wildfires evacuation or...   \n",
       "4   7  nothing  unknown  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding values to keyword and location\n",
    "dataset.keyword = dataset.keyword.apply(lambda x: \"nothing\" if pd.isna(x) else x)\n",
    "dataset.location = dataset.location.apply(lambda x: \"unknown\" if pd.isna(x) else x)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "    text = str(text).lower()\n",
    "\n",
    "    text = re.sub(r'https?://\\S+', ' URL ', text)\n",
    "    text = re.sub(r'@([A-Za-z0-9_]+)', ' Mention ', text)\n",
    "    text = re.sub(r'!', ' E_M ', text) # Exclamation mark\n",
    "    text = re.sub(r'\\?', ' Q_M ', text) # Question mark\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' UncnownCaracters ', text)\n",
    "    text = re.sub(r'[^a-zA-Z# -]', '', text)\n",
    "    text = re.sub(r'[\\n\\t\\r]', ' ', text)\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    \"\"\" text= re.sub(\"[\" u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                     u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                     u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                     u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                     u\"\\U00002702-\\U000027B0\"\n",
    "                     u\"\\U000024C2-\\U0001F251\" \"]+\",'',text,flags=re.UNICODE)\n",
    "    text = re.sub('\\[.*?\\]', '', text) # remove empty\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', 'URL', text) #urls\n",
    "\n",
    "    text = re.sub('\\n', '', text) # newline\n",
    "    text = re.sub('\\w*\\d\\w*', '', text) # numeric data\n",
    "    text = re.sub('[^\\w\\s]', '', text) \"\"\"\n",
    "\n",
    "    return text\n",
    "\n",
    "def correct_spelling(sentence):\n",
    "    spell = SpellChecker()\n",
    "    corrected_words = []\n",
    "\n",
    "    for word in sentence.split():\n",
    "        if not spell.correction(word) == word:\n",
    "            corrected_words.append(spell.correction(word))\n",
    "        else:\n",
    "            corrected_words.append(word)\n",
    "    return \" \".join(map(str,corrected_words))\n",
    "\n",
    "def process_hashtags(arr):\n",
    "    i = 0\n",
    "    while i < len(arr):\n",
    "        if arr[i] == '#':\n",
    "            arr.pop(i)\n",
    "            if i < len(arr):\n",
    "                arr[i] = '#' + arr[i]\n",
    "        i += 1\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       our deeds are the reason of this #earthquake m...\n",
       "1                   forest fire near la ronge sask canada\n",
       "2       all residents asked to shelter in place are be...\n",
       "3       people receive #wildfires evacuation orders in...\n",
       "4       just got sent this photo from ruby #alaska as ...\n",
       "                              ...                        \n",
       "7608    two giant cranes holding a bridge collapse int...\n",
       "7609    Mention Mention the out of control wild fires ...\n",
       "7610                  m utc QM km s of volcano hawaii URL\n",
       "7611    police investigating after an e-bike collided ...\n",
       "7612    the latest more homes razed by northern califo...\n",
       "Name: text, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['text'] = dataset['text'].apply(lambda x: text_cleaner(x))\n",
    "#dataset['text'] = dataset['text'].apply(lambda x: correct_spelling(x)) #Execute it later!!!\n",
    "dataset['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [our, deeds, are, the, reason, of, this, #eart...\n",
       "1           [forest, fire, near, la, ronge, sask, canada]\n",
       "2       [all, residents, asked, to, shelter, in, place...\n",
       "3       [people, receive, #wildfires, evacuation, orde...\n",
       "4       [just, got, sent, this, photo, from, ruby, #al...\n",
       "                              ...                        \n",
       "7608    [two, giant, cranes, holding, a, bridge, colla...\n",
       "7609    [Mention, Mention, the, out, of, control, wild...\n",
       "7610        [m, utc, QM, km, s, of, volcano, hawaii, URL]\n",
       "7611    [police, investigating, after, an, e-bike, col...\n",
       "7612    [the, latest, more, homes, razed, by, northern...\n",
       "Name: text, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['text'] = dataset['text'].apply(lambda x: word_tokenize(x))\n",
    "dataset['text'] = dataset['text'].apply(process_hashtags)\n",
    "dataset['text']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#rockyfire',\n",
       " 'update',\n",
       " 'california',\n",
       " 'hwy',\n",
       " 'closed',\n",
       " 'in',\n",
       " 'both',\n",
       " 'directions',\n",
       " 'due',\n",
       " 'to',\n",
       " 'lake',\n",
       " 'county',\n",
       " 'fire',\n",
       " '-',\n",
       " '#cafire',\n",
       " '#wildfires']"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.text.iloc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [our, deeds, are, the, reason, of, this, #eart...\n",
       "1           [forest, fire, near, la, ronge, sask, canada]\n",
       "2       [all, residents, asked, to, shelter, in, place...\n",
       "3       [people, receive, #wildfires, evacuation, orde...\n",
       "4       [just, got, sent, this, photo, from, ruby, #al...\n",
       "                              ...                        \n",
       "7608    [two, giant, cranes, holding, a, bridge, colla...\n",
       "7609    [Mention, Mention, the, out, of, control, wild...\n",
       "7610        [m, utc, QM, km, s, of, volcano, hawaii, URL]\n",
       "7611    [police, investigating, after, an, e-bike, col...\n",
       "7612    [the, latest, more, homes, razed, by, northern...\n",
       "Name: text, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "dataset['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [deeds, reason, #earthquake, may, allah, forgi...\n",
       "1           [forest, fire, near, la, ronge, sask, canada]\n",
       "2       [residents, asked, shelter, place, notified, o...\n",
       "3       [people, receive, #wildfires, evacuation, orde...\n",
       "4       [got, sent, photo, ruby, #alaska, smoke, #wild...\n",
       "                              ...                        \n",
       "7608    [two, giant, cranes, holding, bridge, collapse...\n",
       "7609    [Mention, Mention, control, wild, fires, calif...\n",
       "7610                  [utc, QM, km, volcano, hawaii, URL]\n",
       "7611    [police, investigating, e-bike, collided, car,...\n",
       "7612    [latest, homes, razed, northern, california, w...\n",
       "Name: cleaned, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['text'] = dataset['text'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "dataset['cleaned'] = dataset['text']\n",
    "dataset['cleaned']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#rockyfire',\n",
       " 'update',\n",
       " 'california',\n",
       " 'hwy',\n",
       " 'closed',\n",
       " 'directions',\n",
       " 'due',\n",
       " 'lake',\n",
       " 'county',\n",
       " 'fire',\n",
       " '-',\n",
       " '#cafire',\n",
       " '#wildfires']"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.cleaned.iloc[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lematization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [(deeds, NNS), (reason, NN), (#earthquake, VBP...\n",
       "1    [(forest, JJS), (fire, NN), (near, IN), (la, J...\n",
       "2    [(residents, NNS), (asked, VBD), (shelter, JJ)...\n",
       "3    [(people, NNS), (receive, VBP), (#wildfires, N...\n",
       "4    [(got, VBD), (sent, JJ), (photo, NN), (ruby, N...\n",
       "Name: cleaned, dtype: object"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['cleaned'] = dataset['cleaned'].apply(lambda words: pos_tag(words))\n",
    "#dataset['cleaned'] = dataset['cleaned'].apply(lambda words: [pos_tag(word_tokenize(word)) for word in words])\n",
    "dataset['cleaned'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#rockyfire', 'NN'),\n",
       " ('update', 'NN'),\n",
       " ('california', 'NN'),\n",
       " ('hwy', 'NN'),\n",
       " ('closed', 'VBD'),\n",
       " ('directions', 'NNS'),\n",
       " ('due', 'JJ'),\n",
       " ('lake', 'VBP'),\n",
       " ('county', 'NN'),\n",
       " ('fire', 'NN'),\n",
       " ('-', ':'),\n",
       " ('#cafire', 'NN'),\n",
       " ('#wildfires', 'NNS')]"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.cleaned.iloc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def penn_to_wordnet(tag):\n",
    "    if tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    elif tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [deed, reason, #earthquake, may, allah, forgiv...\n",
       "1           [forest, fire, near, la, ronge, sask, canada]\n",
       "2       [resident, ask, shelter, place, notify, office...\n",
       "3       [people, receive, #wildfires, evacuation, orde...\n",
       "4       [get, sent, photo, ruby, #alaska, smoke, #wild...\n",
       "                              ...                        \n",
       "7608    [two, giant, crane, hold, bridge, collapse, ne...\n",
       "7609    [Mention, Mention, control, wild, fire, califo...\n",
       "7610                  [utc, QM, km, volcano, hawaii, URL]\n",
       "7611    [police, investigate, e-bike, collide, car, li...\n",
       "7612    [late, home, raze, northern, california, wildf...\n",
       "Name: cleaned, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "dataset['cleaned'] = dataset['cleaned'].apply(lambda words: [lemmatizer.lemmatize(word[0], pos=penn_to_wordnet(word[1])) for word in words])\n",
    "dataset['cleaned'] = dataset['cleaned'].apply(process_hashtags)\n",
    "dataset['cleaned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#rockyfire',\n",
       " 'update',\n",
       " 'california',\n",
       " 'hwy',\n",
       " 'close',\n",
       " 'direction',\n",
       " 'due',\n",
       " 'lake',\n",
       " 'county',\n",
       " 'fire',\n",
       " '-',\n",
       " '#cafire',\n",
       " '#wildfires']"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.cleaned.iloc[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['deed', 'reason', '#earthquake', 'may', 'allah', 'forgive', 'u']"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.cleaned.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "dataset.text = df.text\n",
    "\n",
    "dataset.to_csv('cleaned_dataset.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
